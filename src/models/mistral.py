import os
from mistralai import Mistral
from dotenv import load_dotenv
from src.helpers.json_utils import write_to_json  # Import the JSON function

# Load environment variables from a .env file
load_dotenv()

# Configure the Mistral client with your API key
api_key = os.getenv("MISTRAL_API_KEY")
model = "open-mistral-nemo"  # Define the model name to use

# Initialize the Mistral client with the API key
client = Mistral(api_key=api_key)


def get_mistral_response(prompt):
    """
    Send a prompt to the Mistral model and return the response.

    :param prompt: The text prompt to be sent to the Mistral model.
    :return: The response generated by the model, stripped of extra whitespace.
    """

    # Send the prompt to the model and get a response
    chat_response = client.chat.complete(
        model=model,
        messages=[
            {"role": "user", "content": prompt}
        ],  # Message structure with user role and prompt
    )
    # Return the response generated by the model, stripping any extra whitespace
    return chat_response.choices[0].message.content.strip()


def process_mistral_prompts(input_file, output_file, num_repeats):
    """
    Process prompts from an input file, get Mistral responses, and save results to a JSON file.

    :param input_file: Path to the input file containing prompts (one per line).
    :param output_file: Path to the output file where results will be saved.
    :param num_repeats: Number of times to repeat each prompt for processing.
    """

    # Read prompts from the input file, assuming one prompt per line
    with open(input_file, "r") as file:
        prompts = file.readlines()

    results = []  # Initialize an empty list to store results
    total_prompts = len(prompts)  # Total number of prompts in the file
    total_tasks = (
        total_prompts * num_repeats
    )  # Total number of tasks to process (prompts * repetitions)

    try:
        # Loop through each prompt in the file
        for index, prompt in enumerate(prompts):
            # Strip leading/trailing whitespace from the prompt
            prompt = prompt.strip()
            if not prompt:
                continue  # Skip empty lines

            # Repeat the prompt processing for the specified number of times (num_repeats)
            for repeat in range(num_repeats):
                # Get the response from the Mistral model for the current prompt
                response = get_mistral_response(prompt)

                # Store the result in the format: [Prompt Number, Repeat Number, Prompt, Response]
                results.append([index + 1, repeat + 1, prompt, response])

                # Calculate the current task number and print progress
                current_task = (index * num_repeats) + repeat + 1
                print(
                    f"Mistral: Processing Prompt {index + 1}/{total_prompts}, Repetition {repeat + 1}/{num_repeats} ({current_task}/{total_tasks} tasks completed)"
                )

    finally:
        # Ensure results are written to the JSON file even if an error occurs
        write_to_json(output_file, results)
